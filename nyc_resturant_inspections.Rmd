---
title: "NYC Resturant Inspections"
author: "David Jackson"
date: "10/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(scales)
library(janitor)
library(readxl)
library(tidytext)
library(janeaustenr)
library(stringr)
data("stop_words")
```

## Inspired by David Robinson's YouTube video
* https://youtu.be/em4FXPf4H-Y

```{r}
rm(list=ls())
resturants <- read_excel("./nyc_restaurantsXLS.xlsx")
resturants <- janitor::clean_names(resturants)
resturants <- na.omit(resturants)
resturants$inspection_date <- lubridate::mdy(resturants$inspection_date)
resturants$score <-as.numeric(resturants$score)
resturants <- resturants %>% rename( text = violation_description)

resturants <- resturants %>% filter(inspection_date >="2014-01-01")
resturants1 <- resturants %>% filter(critical_flag =="Critical" )

```

## Discriptive Stats: Date Range Jan. 2014 - Dec. 2019

```{r}
resturants %>% count(inspection_date,sort =TRUE) %>% 
  ggplot(aes(x=inspection_date,y=n))+ geom_line() +
  geom_smooth(aes(x=inspection_date,y=n),method = "glm")
  labs(title = "Count of Inspections by Date")

```

```{r}
resturants$score <-as.numeric(resturants$score)
resturants %>% ggplot(aes(x=score)) + geom_histogram(binwidth = 20) + labs(title = "Histogram of Inspection Scores")
```
```{r}
resturants %>% count(grade,sort =TRUE) %>%
  ggplot(aes(x=reorder(grade,n),y=n)) + geom_col() +
  labs(title = "Count by Grades")
```
```{r}
ggplot(resturants) + geom_line(aes(x=grade,y=score))
```

## Inspections were after Jan. 2014 and Flagged as Critical

```{r}
resturants1 %>% count(boro,sort =TRUE)  %>%
  ggplot(aes(x=reorder(boro,n),n)) + geom_col() +
  labs(title = "Count of Violations by Boro")
```

```{r}
resturants1 %>% count(violation_code,sort = TRUE) %>%
  top_n(25) %>%
  ggplot(aes(x=reorder(violation_code,n),y=n)) + geom_col() +
  coord_flip() + labs(title = "Top 25: by Violation Code")
```
```{r}
resturants1 %>% count(cuisine_description,sort = TRUE) %>%
  top_n(25) %>%
  ggplot(aes(x=reorder(cuisine_description,n),y=n)) + geom_col() +
  coord_flip() + labs(title = "Top 25 Count by Cuisine")

```

## Begin Text Analysis with tidytext

```{r}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
text_df <- tibble(line = 1:4, text = text)

text_tokins <- text_df %>%
  unnest_tokens(word, text)
barplot(table(text_tokins$word))

```

```{r}
text_df <- resturants1 %>% select(text)
text_df$line <- row_number(text_df)

text_tokins <- text_df %>%
  unnest_tokens(word, text)

```

```{r}
data(stop_words)
tidy_stop <- text_tokins %>%
  anti_join(stop_words)
```

```{r}
tidy_stop %>%
  count(word, sort = TRUE) 
```

## First Visualizations

```{r}
tidy_stop %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_point() +
  labs(y = NULL)
```

### Using Ngrams = 2

```{r}
data("stop_words")
text_tokins <- text_df %>%
  unnest_tokens(ngram, text,token = "ngrams",n=4) %>%
  rename(word = ngram)

```
```{r}

tidy_stop <- text_tokins %>%
anti_join(stop_words)

 tidy_stop %>%
  count(word, sort = TRUE) 
```

## Plot of word freq for ngram =2


```{r}
tidy_stop %>% count(word,sort = TRUE) %>% top_n(25) %>%
  ggplot(aes(y=reorder(word,n),x=n)) + geom_point()
```

## Let's get down and dirty with Lexicons

```{r}
get_sentiments("afinn") %>% count(value,sort =T) %>%
  ggplot(aes(x=reorder(value,n),y=n)) + geom_col() +
  coord_flip()

```
```{r}
get_sentiments("bing") %>% count(sentiment,sort =T) %>%
  ggplot(aes(x=reorder(sentiment,n),y=n)) + geom_col() +
  coord_flip()
```
```{r}
get_sentiments("nrc") %>% count(sentiment,sort =T) %>%
  ggplot(aes(x=reorder(sentiment,n),y=n)) + geom_col() +
  coord_flip()
```
```{r}

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```
```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE) %>% top_n(25) %>%
  ggplot(aes(x=reorder(word,n),y=n)) + geom_point() + coord_flip()
```
```{r}
text_df <- resturants1 %>% select(text)
text_df$line <- row_number(text_df)

text_tokins <- text_df %>%
  unnest_tokens(word, text)

```

```{r}
data(stop_words)
tidy_stop <- text_tokins %>%
  anti_join(stop_words)
```

```{r}
tidy_stop %>%
  count(word, sort = TRUE) 
```

#### Lexicon: Bing

```{r}
bing <- get_sentiments("bing")

tidy_bing <- tidy_stop %>% inner_join(bing)

tidy_bing %>% count(sentiment,sort = TRUE) %>%
  ggplot(aes(x=sentiment,y=n)) + geom_col()

```
```{r}
tidy_bing %>% count(word,sentiment,sort = TRUE) %>% top_n(20) %>%
  ggplot(aes(x=reorder(word,n),y=n,fill = sentiment)) + geom_col() +
  facet_wrap( ~sentiment,ncol=1) + coord_flip()

  
```


#### Lexicon  NRC

```{r}
nrc <- get_sentiments("nrc")
tidy_nrc <- tidy_stop %>% inner_join(nrc)
tidy_nrc %>% count(sentiment,sort = TRUE) %>%
  ggplot(aes(x=reorder(sentiment,n),y=n)) + geom_col() +
  coord_flip()
```



### Lexicon: AFINN

```{r}
afinn <- get_sentiments("afinn")
tidy_afinn <- tidy_stop %>% inner_join(afinn)
tidy_afinn %>% count(value,sort = TRUE) %>%
  ggplot(aes(x=reorder(value,n),y=n)) + geom_col()
``` 

### Word Cloud

```{r}
library(wordcloud)
tidy_stop %>%
    count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```



